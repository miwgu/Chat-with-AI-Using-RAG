# Full-stack AI Chatbot with RAG - Docker + Ollama(Linux/WSL2)+ pgvector

## üìö Overview

This system allows users to ask natural language questions and receive intelligent answers generated by an **LLM (Ollama)**.  
By leveraging **Retrieval-Augmented Generation (RAG)**, it retrieves context from your own **knowledge base** stored in PostgreSQL with **pgvector**, enabling accurate and customized responses based on your data.

---
## ‚úÖ Features
- **Minimal, ChatGPT-like UI** ‚Äî simple and intuitive chat interface  
- **Chat history display** ‚Äî messages are shown from oldest to newest  
- **Seamless backend integration** ‚Äî communicates via `/api/query` and `/api/getchatlog`  
- **LLM powered by llama3:8b (via Ollama)**  
- **RAG pipeline implemented with LangChain** ‚Äî retrieves relevant information from a PostgreSQL + pgvector knowledge base before generating responses  
- **Custom knowledge base support** ‚Äî easily store and search your own documents or internal data for context-aware answers  

>  **Ollama** is a lightweight framework that lets you run large language models (LLMs) locally on your own computer ‚Äî without relying on cloud APIs like OpenAI‚Äôs.

---
## üöÄ Tech Stack

- **Frontend:** React (TypeScript + Vite) 
- **Backend:** Express.js (TypeScript)
- **LLM Integration:** LangChain + llama3:8b (via [Ollama](https://ollama.com/))
- **RAG:** Generate and store embeddings with `nomic-embed-text`, retrieve relevant documents using pgvector, and build context prompts for the LLM
- **Database:** PostgreSQL (pgvector)
- **Containerization:** Docker, Docker Compose

---

## üí¨ How It Works
### User Interaction Flow
1. The user types a question in the chat input on the frontend.  
2. The frontend sends a POST request to **`/api/query`**.  
3. The backend processes the request using **LangChain**:  
   - The question is **embedded** using the embedding model (`nomic-embed-text` via Ollama).  
   - A **vector similarity search** is performed in PostgreSQL (pgvector) to find the most relevant context.  
   - The retrieved context is **combined** with the user‚Äôs query and sent to the **LLM (llama3:8b via Ollama)** for response generation.  
4. The generated response is returned to the frontend **and saved in the database** for chat history tracking.  
5. A GET request to **`/api/getchatlog`** retrieves all previous chat logs for display in the UI.

### Under the Hood
- **RAG (Retrieval-Augmented Generation)** implemented with **LangChain**  
- **LLM-powered responses** using **llama3:8b** via **Ollama** (runs locally, no external APIs required)  
- **PostgreSQL + pgvector** for document embeddings and similarity search  
- **Persistent chat history** with unique IDs and timestamps  
- **Frontend rendering** displays messages chronologically (oldest ‚Üí newest)


## üß† System Architecture Diagram

```mermaid
flowchart LR
    %% User
    subgraph User
        A1[Type question in chat UI]
    end

    %% Frontend
    subgraph Frontend
        A2[Frontend React Vite]
        A3[Render chat UI and history]
    end

    %% Backend
    subgraph Backend
        B1[Receive query]
        B2[Embed query via nomic-embed-text]
        B3[Vector search & retrieve relevant context from PostgreSQL pgvector]
        B4[Generate response via llama3:8b Ollama]
        B5[Store question and response in DB]
    end

    %% Database
    subgraph Database
        D1[(Embeddings Table)]
        D2[(Chat Logs Table)]
    end

    %% Ollama
    subgraph Ollama
        O1[(llama3:8b LLM)]
        O2[(nomic-embed-text)]
    end

    %% Docker initialization
    subgraph Docker_Init
        X1[Run init.sql to setup tables]
        X2[registerKnowledge.ts to embed and store knowledge]
    end

    %% Connections
    A1 --> A2
    A2 -->|POST /api/query| B1
    B1 -->|Embedding Request| O2
    O2 --> B2
    B2 --> B3
    B3 -->|Relevant context| B4
    B3 <-->|Context retrieval from pgvector| D1
    B4 <--> O1
    B4 -->|Response| B5
    B5 --> D2
    B5 --> A3
    A3 --> A1

    %% Init connections
    X1 --> D1
    X2 --> D1

```

---

## ‚öôÔ∏è Project Setup Instructions

1. Clone the Repository
```bash
git clone https://github.com/miwgu/Chat-with-AI-Using-RAG.git
```
2. Install Dependencies both frontend and backend
```bash
npm install
``` 
---

## üåçEnviroment file

- Add a .env file in the project root
- change settings for Docker or Local
```bash
#PORT=3000
# FRONTEND_ORIGIN=http://localhost:5173

# Database local settings
#DB_HOST=172.17.112.1
#DB_USER=****
#DB_PASSWORD=****         
#DB_NAME=ai_chat_db2025   
#DB_PORT=3306             


PORT=3000
FRONTEND_ORIGIN=http://localhost:4173
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=mistral
VITE_BACKEND_URL=http://localhost:3000

# Database Docker
DB_HOST=postgres
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_NAME=chatdb

```
-for Frontend
```bash
VITE_BACKEND_URL=http://localhost:3001
```
-for Backend
```bash
FRONTEND_ORIGIN=http://localhost:4173
```


---
## üß† Ollama Setup (Required before running Docker)

Before starting the Docker containers, you need to install and set up **Ollama** on your local machine.  
The backend container connects to Ollama running on your host machine via `http://host.docker.internal:11434`.

   
1. Install Ollama
- For Windows (WSL2 + Ubuntu)  
- For Mac (Intel/Apple Silicon)
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. Download LLM Model (e.g., llama3:8b)
```bash
ollama run llama3:8b
```

3. Download Embed Model (e.g., nomic-embed-text)
```bash
ollama run nomic-embed-text
```

4. Optional: If using a different model, update the model name in ollama.ts.

5. Start Docker
---

## üê≥Docker Container Setup 

1. Start Containers
```bash
docker compose up -d --build
```
> This command automatically builds and starts all containers.
You don‚Äôt need to manually create the database or register knowledge ‚Äî these are handled automatically when the backend container starts.

2. Check backed logs
> Verify that the knowledge base is successfully registered and the server is running.
```bash
docker logs aida-backend
```
3. Stop and Remove Containers (Optional)
```bash
docker compose down
```
>‚ö†Ô∏è Note: The knowledge embeddings are stored in the persistent PostgreSQL volume (postgres_data).
You do not need to re-run the registration script or rebuild the containers unless the volume or database is deleted.
---

## üíæ Database Setup with init.sql
You don‚Äôt need to do anything manually to set up the database `chatdb`.  
It is automatically created by `docker-compose.yml`.

The `postgres_data` volume is **persistent**, which means that even if you delete the container, the volume (and thus the database data) will remain. This is why the database may continue to operate with old data.

```yaml
environment:
  - POSTGRES_DB=${DB_NAME}
  - POSTGRES_USER=${DB_USER}
  - POSTGRES_PASSWORD=${DB_PASSWORD}
volumes:
  - ./init.sql:/docker-entrypoint-initdb.d/init.sql
```
---

## üîÑ Resetting the Database

If you want to completely delete chatdb and create a new one:
```bash
docker compose down -v
```
>This will remove both the container and the persistent volume, so the database is recreated from scratch

- Rebuilds the image from the Dockerfile.
- All latest code and dependency changes are applied.
```bash
docker compose up -d --build
```
OR
- Reuses the existing image.
- Code changes are not reflected in the container.
```bash
docker compose up -d
```

---

## üìù Accessing the Database
Login to the Postgres container (you will need to enter the password):
```bash
docker exec -it aida-postgres-container psql -U myuser -d chatdb
```
Check existing tables:
```bash
\dt  
```


---

## üìà API Endpoints

| Method | Endpoint        | Description                  | Request Body                    | Response Example                  |
|--------|-----------------|------------------------------|--------------------------------|----------------------------------|
| POST   | /api/query      | Send a question to the LLM   | `{ "question": "string" }`      | `{ "response": "string" }`        |
| GET    | /api/getchatlog | Retrieve all chat history    | None                           | `[ { "id": "uuid", "question": "string", "response": "string", "created_at": "timestamp" }, ... ]` |

---

## üìù Future Improvements 

- Move knowledge from Array to JSON or PDF sources for easier management and scalability.
- Add user authentication (JWT) to secure individual chat sessions.
- Allow deleting history entries by ID for better user control.
- Organize chat sessions by topic to make conversation history more structured.
- Add search functionality in chat logs to quickly find previous answers.
- Implement caching (e.g., Redis) to speed up repeated RAG queries.

